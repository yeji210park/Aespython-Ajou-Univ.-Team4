{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Generate images by using RBM on MNIST",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeji210park/Aespython-Ajou-Univ.-Team4/blob/master/Generate_images_by_using_RBM_on_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'digit-recognizer:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F3004%2F861823%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T052839Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dab3a13b1ef1a813bcfbaf5c42babfdcb79741bcee371ff0be54352f40786e3593113eb49615bce19db155c042a019fb9c40fb62efd031188264780764553011780cac18781b76a3289e6d9ebf8d94affc1bc3d23a988c72eed699bafe66d2c1420a2a3ec1dc792a3f037ad909eccbe65456794e2a686de2053148d2430ca6e8f56fa722411f571d6c97bdf5702d77fb88a2abb0b5ff48859bfff5dad29175a93efd77dcdfb3448d0b0579bbf2e8d43e8e872b8e4a8aaa6f9086ad239c52e89d6780621ffed651f23d46ca5214027ecc3fb3f99803dccd3fa0c71ba466addf65376f7aa4ad6182fd6be10d2cf88ae2ef417a0132bac50adadc9d97750d85e8b2d'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zPavUzXquWdH",
        "outputId": "752d43c7-11f0-4692-e13f-9fef4adf7c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading digit-recognizer, 16054568 bytes compressed\n",
            "[==================================================] 16054568 bytes downloaded\n",
            "Downloaded and uncompressed: digit-recognizer\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Generate images by using RBM on MNIST</p>"
      ],
      "metadata": {
        "id": "5jKuGNKquWdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"border-radius:10px;border:#63445d solid;padding: 15px;background-color:white;font-size:110%;text-align:left\">\n",
        "\n",
        "<h3 align=\"left\"><font color=#63445d>What are Boltzmann Machines?</font></h3>\n",
        "    \n",
        "\n",
        "    \n",
        "It is a network of neurons in which all the neurons are connected to each other. In this machine, there are two layers named visible layer or input layer and hidden layer. The visible layer is denoted as v and the hidden layer is denoted as the h. In Boltzmann machine, there is no output layer. Boltzmann machines are random and generative neural networks capable of learning internal representations and are able to represent and (given enough time) solve tough combinatoric problems.\n",
        "\n",
        "The Boltzmann distribution (also known as Gibbs Distribution) which is an integral part of Statistical Mechanics and also explain the impact of parameters like Entropy and Temperature on the Quantum States in Thermodynamics. Due to this, it is also known as Energy-Based Models (EBM). It was invented in 1985 by Geoffrey Hinton, then a Professor at Carnegie Mellon University, and Terry Sejnowski, then a Professor at Johns Hopkins University."
      ],
      "metadata": {
        "id": "6nkfi8DQuWdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "id": "l7eZQVTeuWdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"border-radius:10px;border:#63445d solid;padding: 15px;background-color:white;font-size:110%;text-align:left\">\n",
        "    \n",
        "<h3 align=\"left\"><font color=#63445d>What are Restricted Boltzmann Machines (RBM)?</font></h3>\n",
        "    \n",
        "RBM is a Stochastic Neural Network which means that each neuron will have some random behavior when activated. There are two other layers of bias units (hidden bias and visible bias) in an RBM. This is what makes RBMs different from autoencoders. The hidden bias RBM produces the activation on the forward pass and the visible bias helps RBM to reconstruct the input during a backward pass. The reconstructed input is always different from the actual input as there are no connections among the visible units and therefore, no way of transferring information among themselves."
      ],
      "metadata": {
        "id": "H0F6wvEsuWdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Imort Libraries</p>"
      ],
      "metadata": {
        "id": "9KY2Hd5puWdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time"
      ],
      "metadata": {
        "id": "NdQcjbRouWdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Loading Data</p>"
      ],
      "metadata": {
        "id": "G8BAtXrZuWdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"border-radius:10px;border:#63445d solid;padding: 15px;background-color:white;font-size:110%;text-align:left\">\n",
        "\n",
        "<h3 align=\"left\"><font color=#63445d>about dataset</font></h3>\n",
        "    \n",
        "The MNIST dataset is one of the most well studied datasets in the computer vision and machine learning literature. In many cases, itâ€™s a benchmark, a standard to which some machine learning algorithms are ranked against.\n",
        "The goal of this dataset is to correctly classify the handwritten digits 0-9. We are not going to utilize the entire dataset (which consists of 60,000 training images and 10,000 testing images), instead we are going to utilize a small sample (3,000 for training, 2,000 for testing). The data points are approximately uniformly distributed per digit, so no substantial class label imbalance exists.\n",
        "Each feature vector is 784-dim, corresponding to the 28 x 28 grayscale pixel intensities of the image. These grayscale pixel intensities are unsigned integers, falling into the range [0, 255].\n",
        "All digits are placed on a black background, with the foreground being white and shades of gray.\n"
      ],
      "metadata": {
        "id": "tiIbf5-nuWdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = '/kaggle/input/digit-recognizer/train.csv'\n",
        "TEST_PATH = '/kaggle/input/digit-recognizer/test.csv'\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)"
      ],
      "metadata": {
        "id": "Nyp2TcrTuWdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = (train.iloc[:,1:].values).astype('float32').reshape(-1, 28*28)\n",
        "y = train.iloc[:,0].values.astype('int32')\n",
        "X_test = test.values.astype('float32').reshape(-1, 28*28)"
      ],
      "metadata": {
        "id": "K97PTM-2uWdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTREMELY IMPORTANT TO NORMALIZE\n",
        "X = np.true_divide(X, 255)\n",
        "X_test = np.true_divide(X_test, 255)"
      ],
      "metadata": {
        "id": "St3a-_i4uWdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some hand picked images for each class for plotting\n",
        "selected_imgs = X[[1, 6, 5, 12, 26, 35, 62, 52, 46, 4]]"
      ],
      "metadata": {
        "id": "B8t3IUZbuWdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Train-val split</p>"
      ],
      "metadata": {
        "id": "nrwGoPWZuWdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-val split\n",
        "split = 0.2\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "X_val = X[:int(split*X.shape[0])]\n",
        "X_train = X[int(split*X.shape[0]):]\n",
        "y_val = y[:int(split*X.shape[0])]\n",
        "y_train = y[int(split*X.shape[0]):]\n",
        "print(\"Number of training instances:\\t\", X.shape[0])\n",
        "print(\"Number of validation instances:\\t\", y_val.shape[0])"
      ],
      "metadata": {
        "id": "icf-GPoauWdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Define some Function</p>"
      ],
      "metadata": {
        "id": "FOBemCUguWdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QIfCHt6FuWdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import expit   # sigmoid\n",
        "\n",
        "class RBM():\n",
        "    def __init__(self, n_vis=28*28, n_hid=100):\n",
        "        self.n_vis = n_vis\n",
        "        self.n_hid = n_hid\n",
        "        # Parameters\n",
        "        self.W = 0.1 * np.random.randn(n_vis, n_hid)\n",
        "        self.vbias = np.zeros(n_vis)\n",
        "        self.hbias = -4.0 * np.ones(n_hid)\n",
        "        # Gradients\n",
        "        self.W_grad = np.zeros(self.W.shape)\n",
        "        self.vbias_grad = np.zeros(n_vis)\n",
        "        self.hbias_grad = np.zeros(n_hid)\n",
        "        # Velocities - for momentum\n",
        "        self.W_vel = np.zeros(self.W.shape)\n",
        "        self.vbias_vel = np.zeros(n_vis)\n",
        "        self.hbias_vel = np.zeros(n_hid)\n",
        "\n",
        "    def h_given_v(self, v):\n",
        "        '''\n",
        "        input:\n",
        "            - v: (batch_size, n_vis)\n",
        "        output:\n",
        "            - p(H|v) = sigmoid(W^Tv + hbias): (batch_size, n_hid)\n",
        "            - samples from p(H|v): (batch_size, n_hid)\n",
        "        '''\n",
        "        p = expit(np.matmul(v, self.W) + self.hbias)\n",
        "        return (p, np.random.binomial(1, p=p))\n",
        "\n",
        "    def v_given_h(self, h):\n",
        "        '''\n",
        "        input:\n",
        "            - h: (batch_size, n_hid)\n",
        "        output:\n",
        "            - p(V|h) = sigmoid(Wh + vbias): (batch_size, n_vis)\n",
        "            - samples from p(V|h): (batch_size, n_vis)\n",
        "        '''\n",
        "        p = expit(np.matmul(h, self.W.T) + self.vbias)\n",
        "        return (p, np.random.binomial(1, p=p))\n",
        "\n",
        "    def compute_error_and_grads(self, batch, burn_in=0, num_steps=1, method=\"cd\"):\n",
        "        '''\n",
        "        Function to compute the gradient of parameters and store in param_grad variables\n",
        "        and reconstruction error.\n",
        "        input:\n",
        "            - batch: (batch_size, n_vis)\n",
        "            - burn_in: Number of burn in steps for Gibbs sampling\n",
        "            - num_steps: Number of steps for Gibbs sampling chain to run\n",
        "            - method: Method for computing gradients. Available options:\n",
        "                    - \"cd\": Contrastive Divergence\n",
        "        output:\n",
        "            - recon_error: Reconstruction error\n",
        "\n",
        "        TODO:\n",
        "            - Implement PCD and FPCD.\n",
        "            - Use Gibbs sampling averaging, instead of taking just last value.\n",
        "        '''\n",
        "        b_size = batch.shape[0]\n",
        "        v0 = batch.reshape(b_size, -1)\n",
        "\n",
        "        # Compute gradients - Positive Phase\n",
        "        ph0, h0 = self.h_given_v(v0)\n",
        "        W_grad = np.matmul(v0.T, ph0)\n",
        "        vbias_grad = np.sum(v0, axis=0)\n",
        "        hbias_grad = np.sum(ph0, axis=0)\n",
        "\n",
        "        # Compute gradients - Negative Phase\n",
        "\n",
        "        # only contrastive with k = 1, i.e., method=\"cd\"\n",
        "\n",
        "        pv1, v1 = self.v_given_h(h0)\n",
        "        ph1, h1 = self.h_given_v(pv1)\n",
        "\n",
        "        W_grad -= np.matmul(pv1.T, ph1)\n",
        "        vbias_grad -= np.sum(pv1, axis=0)\n",
        "        hbias_grad -= np.sum(ph1, axis=0)\n",
        "\n",
        "        self.W_grad = W_grad/b_size\n",
        "        self.hbias_grad = hbias_grad/b_size\n",
        "        self.vbias_grad = vbias_grad/b_size\n",
        "\n",
        "        recon_err = np.mean(np.sum((v0 - pv1)**2, axis=1), axis=0) # sum of squared error averaged over the batch\n",
        "        return recon_err\n",
        "\n",
        "    def update_params(self, lr, momentum=0):\n",
        "        '''\n",
        "        Function to update the parameters based on the stored gradients.\n",
        "        input:\n",
        "            - lr: Learning rate\n",
        "            - momentum\n",
        "        '''\n",
        "        self.W_vel *= momentum\n",
        "        self.W_vel += (1.-momentum) * lr * self.W_grad\n",
        "        self.W += self.W_vel\n",
        "\n",
        "        self.vbias_vel *= momentum\n",
        "        self.vbias_vel += (1.-momentum) * lr * self.vbias_grad\n",
        "        self.vbias += self.vbias_vel\n",
        "\n",
        "        self.hbias_vel *= momentum\n",
        "        self.hbias_vel += (1.-momentum) * lr * self.hbias_grad\n",
        "        self.hbias += self.hbias_vel\n",
        "\n",
        "    def reconstruct(self, v):\n",
        "        '''\n",
        "        Reconstructing visible units from given v.\n",
        "        v -> h0 -> v1\n",
        "        input:\n",
        "            - v: (batch_size, n_vis)\n",
        "        output:\n",
        "            - prob of reconstructed v: (batch_size, n_vis)\n",
        "        '''\n",
        "        ph0, h0 = self.h_given_v(v)\n",
        "        pv1, v1 = self.v_given_h(ph0)\n",
        "        return pv1\n",
        "\n",
        "    def avg_free_energy(self, v):\n",
        "        '''\n",
        "        Compute the free energy of v averaged over the batch.\n",
        "        input:\n",
        "            - v: (batch_size, n_vis)\n",
        "        output:\n",
        "            - average of free energy: where free energy = - v.vbias - Sum_j (log(1 + exp(hbias + v_j*W_:,j)) )\n",
        "        '''\n",
        "        x = self.hbias + np.matmul(v, self.W)\n",
        "        free_energy_batch = -np.matmul(v, self.vbias) - np.sum(np.log(1 + np.exp(x)), axis=1)\n",
        "        return np.mean(free_energy_batch)\n",
        "\n",
        "    def gen_model_sample(self, start=None, num_iters=1000):\n",
        "        '''\n",
        "        Generate random samples of visible unit from the model using Gibbs sampling.\n",
        "        input:\n",
        "            - start: Any starting value of v.\n",
        "            - num_iters: Number of iterations of Gibbs sampling.\n",
        "        '''\n",
        "        if(start is None):\n",
        "            v = np.random.randn(self.n_vis)\n",
        "        else:\n",
        "            v = start\n",
        "        for _ in range(num_iters):\n",
        "            ph, h = rbm.h_given_v(v)\n",
        "            pv, v = rbm.v_given_h(h)\n",
        "        return v"
      ],
      "metadata": {
        "id": "yqFu6-CluWdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(data, batch_size, shuffle=False):\n",
        "    '''\n",
        "    Function to provide data in batches.\n",
        "    input:\n",
        "        - data: The data to be batched, each sample in one row.\n",
        "        - batch_size: Size of one batch (last batch might be smaller)\n",
        "        - shuffle: True if data should be shuffled.\n",
        "    '''\n",
        "    if(shuffle):\n",
        "        np.random.shuffle(data)\n",
        "    if(batch_size == -1):\n",
        "        batch_size = len(data)\n",
        "    num_batches = math.ceil(data.shape[0]/batch_size)\n",
        "    for batch_num in range(num_batches):\n",
        "        yield data[batch_num*batch_size:(batch_num+1)*batch_size]"
      ],
      "metadata": {
        "id": "xiB8kCpmuWdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(rbm, images, title='Reconstructed Images', save_as=\"Reconstructed Images\"):\n",
        "    '''\n",
        "    Plot the images and their reconstruction from RBM.\n",
        "    input:\n",
        "        - rbm: The RBM object to be used.\n",
        "        - images: The images to be reconstructed (batch, 28, 28)\n",
        "    '''\n",
        "    num_samples = len(images)\n",
        "    plt.clf()\n",
        "    fig, axes = plt.subplots(2, num_samples, gridspec_kw = {'wspace':0, 'hspace':0.1}, figsize=(2*num_samples, 4))\n",
        "    fig.suptitle(title)\n",
        "    text = axes[0, 0].text(-7, 14,\"original\", size=15,\n",
        "                           verticalalignment='center', rotation=-270)\n",
        "    text = axes[1, 0].text(-7, 13,\"reconstructed\", size=15,\n",
        "                           verticalalignment='center', rotation=-270)\n",
        "    for n in range(num_samples):\n",
        "        axes[0, n].imshow(images[n].reshape(28, 28), cmap='gray')\n",
        "        axes[1, n].imshow(rbm.reconstruct(images[n].reshape(28*28)).reshape(28, 28), cmap='gray')\n",
        "        axes[0, n].axis('off')\n",
        "        axes[1, n].axis('off')\n",
        "\n",
        "    plt.savefig(save_as)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "W9O3WkMzuWdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weights(rbm, title='weights', save_as=\"weights\"):\n",
        "    '''\n",
        "    Plot the weight parameter of the RBM, one for each hidden unit.\n",
        "    '''\n",
        "    plt.clf()\n",
        "    fig, axes = plt.subplots(10, 10, gridspec_kw = {'wspace':0.1, 'hspace':0.1}, figsize=(8, 8))\n",
        "    fig.suptitle(title)\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            axes[i, j].imshow(rbm.W[:,i*10+j].reshape(28, 28), cmap='gray')\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "    plt.savefig(save_as)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "f3kq4sU7uWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">RBM Model</p>"
      ],
      "metadata": {
        "id": "Sh1DoCMcuWdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some parameters to set\n",
        "batch_size = 50\n",
        "num_epochs = 20\n",
        "lr = 0.01\n",
        "burn_in = 0\n",
        "num_steps = 1"
      ],
      "metadata": {
        "id": "I_xQB0PIuWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our RBM object\n",
        "rbm = RBM(n_vis=28*28, n_hid=100)"
      ],
      "metadata": {
        "id": "9sT9fjCPuWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "errors = []\n",
        "free_energies_val = []\n",
        "free_energies_train = []\n",
        "start_time = time()\n",
        "\n",
        "# plot_images(rbm, selected_imgs,\n",
        "#             title=\"Reconstructed Images    Epoch: 0\",\n",
        "#             save_as=\"recon_\" + str(0))\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    iteration = 0\n",
        "    error = 0\n",
        "    for batch in get_batches(X_train, batch_size, shuffle=True):\n",
        "        iteration += 1\n",
        "\n",
        "        # Compute gradients and errors\n",
        "        error += rbm.compute_error_and_grads(batch, burn_in=burn_in, num_steps=num_steps)\n",
        "\n",
        "        # Update parameters - use momentum as explained in Hinton's guide\n",
        "        if(epoch > 5):\n",
        "            rbm.update_params(lr, momentum=0.5)\n",
        "        else:\n",
        "            rbm.update_params(lr, momentum=0.9)\n",
        "\n",
        "    #plot_images(rbm, selected_imgs,\n",
        "    #        title=\"Reconstructed Images    Epoch: {}\".format(epoch),\n",
        "    #        save_as=\"recon_\" + str(epoch))\n",
        "\n",
        "    print(\"epoch:{} \\t error:{:.4f} \\t training time:{:.2f} s\".format(epoch, error, time()-start_time))\n",
        "    errors.append(error)"
      ],
      "metadata": {
        "id": "6An3-Z0CuWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"padding:10px;background-color:#9c8497 ;margin:0;color:white;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Plots</p>"
      ],
      "metadata": {
        "id": "yo_U6IrHuWdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(errors)\n",
        "plt.savefig(\"error_plot\")"
      ],
      "metadata": {
        "id": "qW2qBQMzuWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_weights(rbm)"
      ],
      "metadata": {
        "id": "mA5TP2BsuWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(rbm, selected_imgs)"
      ],
      "metadata": {
        "id": "tK9H7FhMuWdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ba9Ji2FFuWdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}